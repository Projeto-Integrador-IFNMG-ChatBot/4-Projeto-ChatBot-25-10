{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a1128f50-8bc9-4fea-aba0-b51079afedf7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Módulo 'horario_parser' importado com sucesso.\n",
      "PDF 'Ciencia-da-computacao-01-2025_com_sala_MkIII.pdf' encontrado com 7 páginas.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processando Páginas:   0%|                                                                       | 0/7 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Processando Página 1 ---\n",
      "Metadados encontrados: {'semestre': '1/2025', 'turma': '20 INGRESSANTES (MATUTINO)', 'salas_info': 'P2 – Sala 7, P2-LAB CC (quando indicado)'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processando Páginas:  14%|█████████                                                      | 1/7 [00:00<00:04,  1.24it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tabela da página 1 processada com sucesso.\n",
      "\n",
      "--- Processando Página 2 ---\n",
      "Metadados encontrados: {'semestre': '1/2025', 'turma': '30 (MATUTINO)', 'salas_info': 'P2 - Sala 9, P2-LAB CC (quando indicado)'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processando Páginas:  29%|██████████████████                                             | 2/7 [00:01<00:03,  1.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tabela da página 2 processada com sucesso.\n",
      "\n",
      "--- Processando Página 3 ---\n",
      "Metadados encontrados: {'semestre': '1/2025', 'turma': '50 PERÍODO (VESPERTINO)', 'salas_info': 'P2 – sala 9, P2-LAB CC (quando indicado)'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processando Páginas:  43%|███████████████████████████                                    | 3/7 [00:02<00:02,  1.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tabela da página 3 processada com sucesso.\n",
      "\n",
      "--- Processando Página 4 ---\n",
      "Metadados encontrados: {'semestre': '1/2025', 'turma': '70 PERÍODO (MATUTINO)', 'salas_info': 'P2 – Sala 8 ou P2- LAB CC ou P2-LabRedes(quando indicado)'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processando Páginas:  57%|████████████████████████████████████                           | 4/7 [00:02<00:02,  1.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tabela da página 4 processada com sucesso.\n",
      "\n",
      "--- Processando Página 5 ---\n",
      "Metadados encontrados: {'semestre': '1/2025', 'turma': '90 PERÍODO / Optativas', 'salas_info': 'P2 – Sala 9, P2-LAB CC ou P2-LAB Redes (quando indicados)'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\extre\\OneDrive\\Área de Trabalho\\preprocessamento-pdfs-master\\notebooks\\..\\src\\horario_parser.py:216: UserWarning: DataFrame columns are not unique, some columns will be omitted.\n",
      "  \"horario\": processed_df.to_dict(orient='records') # Converte DataFrame processado para lista de dicts\n",
      "Processando Páginas:  71%|█████████████████████████████████████████████                  | 5/7 [00:03<00:01,  1.43it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tabela da página 5 processada com sucesso.\n",
      "\n",
      "--- Processando Página 6 ---\n",
      "Metadados encontrados: {'semestre': '1/2025', 'turma': 'Optativas (Vespertino)', 'salas_info': 'P2 – Sala 7, P2-LAB CC ou P2-LAB Redes (quando indicados)'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processando Páginas:  86%|██████████████████████████████████████████████████████         | 6/7 [00:04<00:00,  1.50it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tabela da página 6 processada com sucesso.\n",
      "\n",
      "--- Processando Página 7 ---\n",
      "Metadados encontrados: {'semestre': '1/2025', 'turma': 'RE-OFERTAS  (Vespertino)', 'salas_info': 'P2 – sala 7,P2-LAB CC (quando indicado)'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processando Páginas: 100%|███████████████████████████████████████████████████████████████| 7/7 [00:04<00:00,  1.44it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tabela da página 7 processada com sucesso.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "keys must be str, int, float, bool or None, not NAType",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 67\u001b[39m\n\u001b[32m     64\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(output_path, \u001b[33m'\u001b[39m\u001b[33mw\u001b[39m\u001b[33m'\u001b[39m, encoding=\u001b[33m'\u001b[39m\u001b[33mutf-8\u001b[39m\u001b[33m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[32m     65\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m schedule \u001b[38;5;129;01min\u001b[39;00m all_extracted_schedules:\n\u001b[32m     66\u001b[39m         \u001b[38;5;66;03m# Escreve cada horário extraído como uma linha JSON separada (JSON Lines)\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m67\u001b[39m         f.write(\u001b[43mjson\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdumps\u001b[49m\u001b[43m(\u001b[49m\u001b[43mschedule\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mensure_ascii\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m + \u001b[33m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m'\u001b[39m)\n\u001b[32m     68\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mExtração concluída com sucesso!\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     69\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(all_extracted_schedules)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m horários foram extraídos e salvos em: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00moutput_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\json\\__init__.py:238\u001b[39m, in \u001b[36mdumps\u001b[39m\u001b[34m(obj, skipkeys, ensure_ascii, check_circular, allow_nan, cls, indent, separators, default, sort_keys, **kw)\u001b[39m\n\u001b[32m    232\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcls\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    233\u001b[39m     \u001b[38;5;28mcls\u001b[39m = JSONEncoder\n\u001b[32m    234\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[32m    235\u001b[39m \u001b[43m    \u001b[49m\u001b[43mskipkeys\u001b[49m\u001b[43m=\u001b[49m\u001b[43mskipkeys\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mensure_ascii\u001b[49m\u001b[43m=\u001b[49m\u001b[43mensure_ascii\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    236\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcheck_circular\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcheck_circular\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mallow_nan\u001b[49m\u001b[43m=\u001b[49m\u001b[43mallow_nan\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindent\u001b[49m\u001b[43m=\u001b[49m\u001b[43mindent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    237\u001b[39m \u001b[43m    \u001b[49m\u001b[43mseparators\u001b[49m\u001b[43m=\u001b[49m\u001b[43mseparators\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdefault\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdefault\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msort_keys\u001b[49m\u001b[43m=\u001b[49m\u001b[43msort_keys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m--> \u001b[39m\u001b[32m238\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mencode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\json\\encoder.py:200\u001b[39m, in \u001b[36mJSONEncoder.encode\u001b[39m\u001b[34m(self, o)\u001b[39m\n\u001b[32m    196\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m encode_basestring(o)\n\u001b[32m    197\u001b[39m \u001b[38;5;66;03m# This doesn't pass the iterator directly to ''.join() because the\u001b[39;00m\n\u001b[32m    198\u001b[39m \u001b[38;5;66;03m# exceptions aren't as detailed.  The list call should be roughly\u001b[39;00m\n\u001b[32m    199\u001b[39m \u001b[38;5;66;03m# equivalent to the PySequence_Fast that ''.join() would do.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m200\u001b[39m chunks = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43miterencode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mo\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_one_shot\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m    201\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(chunks, (\u001b[38;5;28mlist\u001b[39m, \u001b[38;5;28mtuple\u001b[39m)):\n\u001b[32m    202\u001b[39m     chunks = \u001b[38;5;28mlist\u001b[39m(chunks)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\json\\encoder.py:261\u001b[39m, in \u001b[36mJSONEncoder.iterencode\u001b[39m\u001b[34m(self, o, _one_shot)\u001b[39m\n\u001b[32m    256\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    257\u001b[39m     _iterencode = _make_iterencode(\n\u001b[32m    258\u001b[39m         markers, \u001b[38;5;28mself\u001b[39m.default, _encoder, indent, floatstr,\n\u001b[32m    259\u001b[39m         \u001b[38;5;28mself\u001b[39m.key_separator, \u001b[38;5;28mself\u001b[39m.item_separator, \u001b[38;5;28mself\u001b[39m.sort_keys,\n\u001b[32m    260\u001b[39m         \u001b[38;5;28mself\u001b[39m.skipkeys, _one_shot)\n\u001b[32m--> \u001b[39m\u001b[32m261\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_iterencode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mo\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mTypeError\u001b[39m: keys must be str, int, float, bool or None, not NAType"
     ]
    }
   ],
   "source": [
    "# notebooks/processar_horarios.ipynb\n",
    "\n",
    "import sys\n",
    "import os\n",
    "import fitz # Importar fitz aqui também para pegar o número de páginas\n",
    "import json\n",
    "from tqdm import tqdm # Para barra de progresso\n",
    "\n",
    "# Garante que o Python encontre a pasta 'src' que está um nível acima\n",
    "if '..' not in sys.path:\n",
    "    sys.path.append('..')\n",
    "\n",
    "# Importa a função principal que criamos no novo arquivo .py\n",
    "try:\n",
    "    from src.horario_parser import extract_schedule_from_page\n",
    "    print(\"Módulo 'horario_parser' importado com sucesso.\")\n",
    "except ImportError as e:\n",
    "    print(f\"Erro ao importar 'horario_parser': {e}\")\n",
    "    print(\"Verifique se o arquivo 'src/horario_parser.py' existe e não contém erros de sintaxe.\")\n",
    "    # Interrompe a execução se não conseguir importar\n",
    "    raise\n",
    "\n",
    "# --- Configurações ---\n",
    "# Coloque o nome (ou caminho relativo desde a raiz do projeto) do seu PDF de horário aqui\n",
    "pdf_input_path = \"data/input/Ciencia-da-computacao-01-2025_com_sala_MkIII.pdf\"\n",
    "# Nome do arquivo de saída na pasta data/output\n",
    "output_filename = \"horarios_extraidos_CienciaComp.jsonl\"\n",
    "output_path = os.path.join(\"..\", \"data\", \"output\", output_filename) # Caminho relativo desde 'notebooks/'\n",
    "\n",
    "# Cria a pasta de saída se não existir\n",
    "os.makedirs(os.path.dirname(output_path), exist_ok=True)\n",
    "\n",
    "# --- Processamento ---\n",
    "all_extracted_schedules = []\n",
    "num_pages = 0\n",
    "\n",
    "if not os.path.exists(os.path.join(\"..\", pdf_input_path)): # Verifica se o PDF existe (caminho relativo desde notebooks/)\n",
    "     print(f\"Erro: Arquivo PDF não encontrado em '{pdf_input_path}'. Verifique o caminho.\")\n",
    "else:\n",
    "    try:\n",
    "        # Abre o PDF brevemente só para contar as páginas\n",
    "        with fitz.open(os.path.join(\"..\", pdf_input_path)) as doc:\n",
    "            num_pages = len(doc)\n",
    "        print(f\"PDF '{os.path.basename(pdf_input_path)}' encontrado com {num_pages} páginas.\")\n",
    "\n",
    "        # Itera por cada página do PDF\n",
    "        if num_pages > 0:\n",
    "            for page_number in tqdm(range(1, num_pages + 1), desc=\"Processando Páginas\"):\n",
    "                # Chama a função principal do nosso módulo para processar a página\n",
    "                schedule_data = extract_schedule_from_page(os.path.join(\"..\", pdf_input_path), page_number)\n",
    "\n",
    "                # Se a função retornou dados válidos, adiciona à lista\n",
    "                if schedule_data:\n",
    "                    all_extracted_schedules.append(schedule_data)\n",
    "                else:\n",
    "                    print(f\"Nenhum horário válido extraído ou erro na página {page_number}.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Erro inesperado durante o processamento do PDF: {e}\")\n",
    "\n",
    "# --- Salvando Resultados ---\n",
    "if all_extracted_schedules:\n",
    "    try:\n",
    "        with open(output_path, 'w', encoding='utf-8') as f:\n",
    "            for schedule in all_extracted_schedules:\n",
    "                # Escreve cada horário extraído como uma linha JSON separada (JSON Lines)\n",
    "                f.write(json.dumps(schedule, ensure_ascii=False) + '\\n')\n",
    "        print(f\"\\nExtração concluída com sucesso!\")\n",
    "        print(f\"{len(all_extracted_schedules)} horários foram extraídos e salvos em: {output_path}\")\n",
    "    except IOError as e:\n",
    "        print(f\"Erro ao salvar o arquivo de saída '{output_path}': {e}\")\n",
    "else:\n",
    "    print(\"\\nNenhum horário foi extraído. O arquivo de saída não foi gerado.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fb74446-f8c4-4a1f-9c6d-a8ce3add5404",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

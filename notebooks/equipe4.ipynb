{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d7d20556-799d-4f28-b281-0455dd20143d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Módulos e funções importados com sucesso.\n"
     ]
    }
   ],
   "source": [
    "# ==============================================================================\n",
    "# 1. SETUP E IMPORTAÇÕES\n",
    "# ==============================================================================\n",
    "import sys\n",
    "import glob\n",
    "import json\n",
    "import os\n",
    "import fitz  # PyMuPDF\n",
    "from tqdm import tqdm # Para barras de progresso\n",
    "\n",
    "# Adiciona o diretório raiz ao path\n",
    "if '..' not in sys.path:\n",
    "    sys.path.append('..')\n",
    "\n",
    "# Importa todas as nossas funções especializadas de 'src'\n",
    "from src.image_extractor import extract_images_from_pdf\n",
    "from src.table_extractor import extract_raw_dataframe\n",
    "from src.table_enhancer import enhance_table             # Parser \"Genérico\" (ou de Calendário)\n",
    "from src.horario_parser import extract_schedule_from_page # Parser de Horários\n",
    "from src.ppc_parser import parse_ppc_page                 # >>> NOVO: Parser de PPC\n",
    "\n",
    "from src.text_normalization import normalize_text\n",
    "from src.structure_detector import detect_structure\n",
    "from src.deduplicator import deduplicate_chunks\n",
    "from src.metadata_enricher import enrich_with_metadata\n",
    "\n",
    "print(\"Módulos e funções importados com sucesso.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cbefad0b-f8e5-40c2-91bc-4e130e1ae9e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dicionário de siglas carregado de '../data/acronyms.json'.\n"
     ]
    }
   ],
   "source": [
    "# ==============================================================================\n",
    "# 2. DEFINIÇÃO DE CAMINHOS E CONFIGURAÇÕES\n",
    "# ==============================================================================\n",
    "# Caminhos de entrada e saída\n",
    "input_dir = '../data/input'\n",
    "output_dir = '../data/output'\n",
    "images_output_dir = os.path.join(output_dir, 'images') # Subpasta para imagens\n",
    "\n",
    "# Cria os diretórios de saída se eles não existirem\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "os.makedirs(images_output_dir, exist_ok=True)\n",
    "\n",
    "# Carrega o dicionário de siglas de um arquivo de configuração externo\n",
    "acronyms_path = '../data/acronyms.json'\n",
    "try:\n",
    "    # Corrigido: Removido espaço estranho antes de with\n",
    "    with open(acronyms_path, 'r', encoding='utf-8') as f:\n",
    "        dicionario_de_siglas = json.load(f)\n",
    "    print(f\"Dicionário de siglas carregado de '{acronyms_path}'.\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"Aviso: Arquivo de siglas '{acronyms_path}' não encontrado. A expansão não será realizada.\")\n",
    "    dicionario_de_siglas = {}\n",
    "except json.JSONDecodeError as e: # Adicionado: Captura erro de JSON mal formatado\n",
    "    print(f\"Erro ao ler o arquivo JSON de siglas '{acronyms_path}': {e}. A expansão não será realizada.\")\n",
    "    dicionario_de_siglas = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "124ce704-df8b-4304-a2e9-e6972c8cc9c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Iniciando o processamento de 2 arquivo(s) PDF...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processando Documentos:   0%|                                                                    | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processando: Ciencia-da-computacao-01-2025_com_sala_MkIII.pdf\n",
      "--> Detectado como tipo 'schedule' (horário).\n",
      "\n",
      "--- Processando Página 1 ---\n",
      "Metadados encontrados: {'semestre': '1/2025', 'turma': '20 INGRESSANTES (MATUTINO)', 'salas_info': 'P2 – Sala 7, P2-LAB CC (quando indicado)'}\n",
      "----> Sala padrão detectada: 'P2 – Sala 7'\n",
      "Tabela da página 1 processada com sucesso (salas vazias preenchidas).\n",
      "\n",
      "--- Processando Página 2 ---\n",
      "Metadados encontrados: {'semestre': '1/2025', 'turma': '30 (MATUTINO)', 'salas_info': 'P2 - Sala 9, P2-LAB CC (quando indicado)'}\n",
      "----> Sala padrão detectada: 'P2 - Sala 9'\n",
      "Tabela da página 2 processada com sucesso (salas vazias preenchidas).\n",
      "\n",
      "--- Processando Página 3 ---\n",
      "Metadados encontrados: {'semestre': '1/2025', 'turma': '50 PERÍODO (VESPERTINO)', 'salas_info': 'P2 – sala 9, P2-LAB CC (quando indicado)'}\n",
      "----> Sala padrão detectada: 'P2 – sala 9'\n",
      "Tabela da página 3 processada com sucesso (salas vazias preenchidas).\n",
      "\n",
      "--- Processando Página 4 ---\n",
      "Metadados encontrados: {'semestre': '1/2025', 'turma': '70 PERÍODO (MATUTINO)', 'salas_info': 'P2 – Sala 8 ou P2- LAB CC ou P2-LabRedes(quando indicado)'}\n",
      "----> Sala padrão detectada: 'P2 – Sala 8'\n",
      "Tabela da página 4 processada com sucesso (salas vazias preenchidas).\n",
      "\n",
      "--- Processando Página 5 ---\n",
      "Metadados encontrados: {'semestre': '1/2025', 'turma': '90 PERÍODO / Optativas', 'salas_info': 'P2 – Sala 9, P2-LAB CC ou P2-LAB Redes (quando indicados)'}\n",
      "----> Sala padrão detectada: 'P2 – Sala 9'\n",
      "Tabela da página 5 processada com sucesso (salas vazias preenchidas).\n",
      "\n",
      "--- Processando Página 6 ---\n",
      "Metadados encontrados: {'semestre': '1/2025', 'turma': 'Optativas (Vespertino)', 'salas_info': 'P2 – Sala 7, P2-LAB CC ou P2-LAB Redes (quando indicados)'}\n",
      "----> Sala padrão detectada: 'P2 – Sala 7'\n",
      "Tabela da página 6 processada com sucesso (salas vazias preenchidas).\n",
      "\n",
      "--- Processando Página 7 ---\n",
      "Metadados encontrados: {'semestre': '1/2025', 'turma': 'RE-OFERTAS  (Vespertino)', 'salas_info': 'P2 – sala 7,P2-LAB CC (quando indicado)'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processando Documentos:  50%|██████████████████████████████                              | 1/2 [00:03<00:03,  3.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----> Sala padrão detectada: 'P2 – sala 7'\n",
      "Tabela da página 7 processada com sucesso (salas vazias preenchidas).\n",
      "---> PREPARANDO PARA SALVAR Ciencia-da-computacao-01-2025_com_sala_MkIII.pdf (Tipo: schedule):\n",
      "     Metadata keys: ['doc_id', 'nome_doc', 'schedules']\n",
      "     page_specific_data length: 7\n",
      "     content_chunks length: 1\n",
      "---> Arquivo JSONL salvo com sucesso (formatado com indentação): ../data/output\\Ciencia-da-computacao-01-2025_com_sala_MkIII.pdf.jsonl\n",
      "\n",
      "Processando: PPCBCC2019.pdf\n",
      "--> Detectado como tipo 'ppc' (Projeto Pedagógico). Keywords: ['projeto pedagógico']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\extre\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\camelot\\utils.py:1217: UserWarning:   (546.36, 549.36) does not lie in column range (50.14801655078598, 546.009939540508)\n",
      "  warnings.warn(\n",
      "C:\\Users\\extre\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\camelot\\utils.py:1217: UserWarning:   (546.36, 549.36) does not lie in column range (50.149422517382106, 546.009939540508)\n",
      "  warnings.warn(\n",
      "C:\\Users\\extre\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\camelot\\utils.py:1217: UserWarning:   (546.36, 549.36) does not lie in column range (50.14895386185007, 546.009939540508)\n",
      "  warnings.warn(\n",
      "C:\\Users\\extre\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\camelot\\utils.py:1217: UserWarning:   (546.36, 549.36) does not lie in column range (50.149422517382106, 546.009939540508)\n",
      "  warnings.warn(\n",
      "C:\\Users\\extre\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\camelot\\utils.py:1217: UserWarning:   (546.36, 549.36) does not lie in column range (50.149422517382106, 546.009939540508)\n",
      "  warnings.warn(\n",
      "C:\\Users\\extre\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\camelot\\utils.py:1217: UserWarning:   (546.36, 549.36) does not lie in column range (50.14895386185007, 546.009939540508)\n",
      "  warnings.warn(\n",
      "C:\\Users\\extre\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\camelot\\utils.py:1217: UserWarning:   (546.36, 549.36) does not lie in column range (50.149422517382106, 546.009939540508)\n",
      "  warnings.warn(\n",
      "C:\\Users\\extre\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\camelot\\utils.py:1217: UserWarning:   (546.36, 549.36) does not lie in column range (50.149422517382106, 546.009939540508)\n",
      "  warnings.warn(\n",
      "C:\\Users\\extre\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\camelot\\utils.py:1217: UserWarning:   (546.36, 549.36) does not lie in column range (50.14895386185007, 546.009939540508)\n",
      "  warnings.warn(\n",
      "C:\\Users\\extre\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\camelot\\utils.py:1217: UserWarning:   (546.36, 549.36) does not lie in column range (50.146141928657805, 546.009939540508)\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Info PPC_PARSER: _parse_optativas não encontrou 'DISCIPLINA'. Tratando como página de continuação.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processando Documentos: 100%|████████████████████████████████████████████████████████████| 2/2 [01:10<00:00, 35.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---> PREPARANDO PARA SALVAR PPCBCC2019.pdf (Tipo: ppc):\n",
      "     Metadata keys: ['doc_id', 'nome_doc']\n",
      "     page_specific_data length: 103\n",
      "     content_chunks length: 1\n",
      "---> Arquivo JSONL salvo com sucesso (formatado com indentação): ../data/output\\PPCBCC2019.pdf.jsonl\n",
      "\n",
      "----------------------------------------------------\n",
      "Processamento de todos os arquivos concluído!\n",
      "Resultados salvos em: '../data/output'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# ==============================================================================\n",
    "# 3. EXECUÇÃO DO PIPELINE COMPLETO (VERSÃO FINAL COM DETECÇÃO AVANÇADA DE TIPO)\n",
    "# ==============================================================================\n",
    "import pandas as pd\n",
    "import fitz\n",
    "import os\n",
    "import json\n",
    "\n",
    "# Encontra todos os arquivos PDF na pasta de entrada\n",
    "pdf_files = glob.glob(os.path.join(input_dir, '*.pdf'))\n",
    "print(f\"\\nIniciando o processamento de {len(pdf_files)} arquivo(s) PDF...\")\n",
    "\n",
    "# Itera sobre cada arquivo PDF encontrado\n",
    "for pdf_path in tqdm(pdf_files, desc=\"Processando Documentos\"):\n",
    "    file_name = os.path.basename(pdf_path)\n",
    "    print(f\"\\nProcessando: {file_name}\")\n",
    "\n",
    "    # --- >>> INÍCIO: Detecção Automática de Tipo de PDF por Conteúdo <<< ---\n",
    "    pdf_type = \"generic\" # Começa com o padrão\n",
    "    try:\n",
    "        with fitz.open(pdf_path) as temp_doc:\n",
    "            if len(temp_doc) > 0:\n",
    "                # Analisa o texto da primeira página em minúsculas\n",
    "                first_page_text = temp_doc.load_page(0).get_text(\"text\").lower()\n",
    "\n",
    "                # 1. Lista de keywords para PPC (Prioridade 1)\n",
    "                ppc_keywords = [\"projeto pedagógico\", \"matriz curricular\", \"ementário\", \"colegiado de curso\", \"ppcbcc\",\"Projeto Pedagógico\"]\n",
    "                # 2. Lista de keywords para Horário (Prioridade 2)\n",
    "                horario_keywords = [\"horário\", \"segunda\", \"terça\", \"quarta\", \"quinta\", \"sexta\", \"manhã\", \"tarde\"]\n",
    "                \n",
    "                # --- Lógica de Prioridade ---\n",
    "                # 1. Checa se é PPC (keywords mais fortes)\n",
    "                ppc_match_count = sum(1 for kw in ppc_keywords if kw in first_page_text)\n",
    "                if ppc_match_count >= 1: # Se 2 ou mais keywords de PPC baterem\n",
    "                    pdf_type = \"ppc\"\n",
    "                    print(f\"--> Detectado como tipo 'ppc' (Projeto Pedagógico). Keywords: {[kw for kw in ppc_keywords if kw in first_page_text]}\")\n",
    "                \n",
    "                # 2. Se NÃO for PPC, checa se é Horário\n",
    "                elif sum(1 for kw in horario_keywords if kw in first_page_text) >= 5: # Usando seu threshold de 5\n",
    "                    pdf_type = \"schedule\"\n",
    "                    print(f\"--> Detectado como tipo 'schedule' (horário).\")\n",
    "                \n",
    "                else:\n",
    "                    # Se não for nenhum dos dois, continua como \"generic\"\n",
    "                    print(f\"--> Detectado como tipo 'generic' (lógica antiga/padrão será usada).\")\n",
    "            else:\n",
    "                 print(\"--> PDF vazio, tratando como 'generic'.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Alerta: Não foi possível analisar conteúdo de '{file_name}': {e}. Tratando como 'generic'.\")\n",
    "    # --- >>> FIM: Detecção Automática de Tipo de PDF <<< ---\n",
    "\n",
    "\n",
    "    # --- ETAPA DE EXTRAÇÃO (NÍVEL DO DOCUMENTO) ---\n",
    "    all_raw_text = \"\"\n",
    "    page_level_data = [] # Lista temporária para dados de página\n",
    "    metadata = {         # Metadados base (serão atualizados depois)\n",
    "        \"doc_id\": file_name.replace('.pdf', ''),\n",
    "        \"nome_doc\": file_name,\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        # Abre o PDF principal para processamento completo\n",
    "        with fitz.open(pdf_path) as doc:\n",
    "            num_pages = doc.page_count\n",
    "            for page_num in range(num_pages):\n",
    "                page = doc.load_page(page_num)\n",
    "                page_1_indexed = page_num + 1\n",
    "\n",
    "                # Extração de texto bruto (para content_chunks)\n",
    "                page_text = page.get_text(\"text\")\n",
    "                if page_text:\n",
    "                     all_raw_text += page_text + \"\\n\\n\"\n",
    "\n",
    "                # Extração de Imagens (comum a todos)\n",
    "                image_info = extract_images_from_pdf(pdf_path, page_1_indexed, images_output_dir)\n",
    "\n",
    "                # --- >>> INÍCIO: LÓGICA CONDICIONAL DE PROCESSAMENTO <<< ---\n",
    "                \n",
    "                if pdf_type == \"schedule\":\n",
    "                    # --- LÓGICA PARA HORÁRIO (Chama horario_parser.py) ---\n",
    "                    schedule_page_data = extract_schedule_from_page(pdf_path, page_1_indexed)\n",
    "                    page_data_entry = {\"page\": page_1_indexed, \"page_type\": \"schedule\", \"images\": image_info}\n",
    "                    if schedule_page_data:\n",
    "                        schedule_page_data.pop(\"pagina\", None)\n",
    "                        page_data_entry.update(schedule_page_data)\n",
    "                    else:\n",
    "                        page_data_entry[\"error\"] = \"Falha na extração/processamento do horário nesta página.\"\n",
    "                    page_level_data.append(page_data_entry)\n",
    "\n",
    "                elif pdf_type == \"ppc\":\n",
    "                    # --- LÓGICA PARA PPC (Chama ppc_parser.py) ---\n",
    "                    # (Usando nosso placeholder por enquanto)\n",
    "                    ppc_page_data = parse_ppc_page(pdf_path, page_1_indexed)\n",
    "                    page_data_entry = {\"page\": page_1_indexed, \"images\": image_info}\n",
    "                    page_data_entry.update(ppc_page_data) # Adiciona 'page_type', 'tables', etc. do parser\n",
    "                    page_level_data.append(page_data_entry)\n",
    "\n",
    "                else: # pdf_type == \"generic\"\n",
    "                    # --- LÓGICA GENÉRICA (ANTIGA / table_enhancer.py) ---\n",
    "                    raw_df = extract_raw_dataframe(pdf_path, page=page_1_indexed)\n",
    "                    enhanced_table_info = {}\n",
    "                    if not raw_df.empty:\n",
    "                        try:\n",
    "                            enhanced_table_info = enhance_table(raw_df)\n",
    "                        except Exception as table_enhance_error:\n",
    "                             print(f\"\\nAlerta: Erro ao aprimorar tabela genérica na pág {page_1_indexed}: {table_enhance_error}\")\n",
    "                             enhanced_table_info = {\"error\": str(table_enhance_error)}\n",
    "                    \n",
    "                    page_level_data.append({\n",
    "                        \"page\": page_1_indexed,\n",
    "                        \"page_type\": \"generic\",\n",
    "                        \"tables\": [enhanced_table_info.get(\"cleaned_table\", [])] if \"error\" not in enhanced_table_info else [{\"error\": enhanced_table_info.get(\"error\")}],\n",
    "                        \"table_legends\": [enhanced_table_info.get(\"legend\", \"\")] if \"error\" not in enhanced_table_info else [],\n",
    "                        \"table_summaries\": [enhanced_table_info.get(\"summary\", \"\")] if \"error\" not in enhanced_table_info else [],\n",
    "                        \"images\": image_info\n",
    "                    })\n",
    "                # --- >>> FIM: LÓGICA CONDICIONAL DE PROCESSAMENTO <<< ---\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"\\nERRO CRÍTICO ao processar páginas de '{file_name}'. Pulando para próximo arquivo. Detalhes: {e}\")\n",
    "        continue # Pula para o próximo arquivo PDF\n",
    "\n",
    "    # --- ETAPAS DE PROCESSAMENTO (TEXTO COMPLETO DO DOCUMENTO) ---\n",
    "    final_chunks = []\n",
    "    if all_raw_text:\n",
    "        try:\n",
    "            normalized_text = normalize_text(all_raw_text, acronyms=dicionario_de_siglas)\n",
    "            structured_chunks = detect_structure(normalized_text) # (Ainda não otimizado para PPC)\n",
    "            unique_chunks = deduplicate_chunks(structured_chunks)\n",
    "        except Exception as text_processing_error:\n",
    "             print(f\"Alerta: Erro no processamento de texto completo para '{file_name}': {text_processing_error}\")\n",
    "             unique_chunks = [{\"error\": \"Falha no processamento do texto completo\", \"details\": str(text_processing_error)}]\n",
    "    else:\n",
    "        print(f\"Alerta: Nenhum texto bruto extraído de '{file_name}'.\")\n",
    "        unique_chunks = [{\"error\": \"Nenhum texto bruto extraído do PDF\"}]\n",
    "\n",
    "\n",
    "    # --- AJUSTE PARA MOVER HORÁRIOS PARA METADATA ---\n",
    "    schedules_found = []\n",
    "    final_page_specific_data = []\n",
    "\n",
    "    for page_data in page_level_data:\n",
    "        if page_data.get(\"page_type\") == \"schedule\" and \"horario\" in page_data and page_data.get(\"horario\") is not None and \"error\" not in page_data:\n",
    "            schedule_info = {\n",
    "                \"pagina_origem\": page_data.get(\"page\"),\n",
    "                \"semestre\": page_data.get(\"semestre\"),\n",
    "                \"turma\": page_data.get(\"turma\"),\n",
    "                \"salas_info\": page_data.get(\"salas_info\"),\n",
    "                \"horario\": page_data.get(\"horario\")\n",
    "            }\n",
    "            schedules_found.append(schedule_info)\n",
    "            final_page_specific_data.append({\n",
    "                \"page\": page_data.get(\"page\"),\n",
    "                \"page_type\": \"schedule_processed\",\n",
    "                \"images\": page_data.get(\"images\", [])\n",
    "            })\n",
    "        else:\n",
    "            final_page_specific_data.append(page_data) # Mantém PPC, Genérico, ou erros de horário\n",
    "\n",
    "    if schedules_found:\n",
    "      metadata[\"schedules\"] = schedules_found\n",
    "    # --- FIM AJUSTE METADATA ---\n",
    "\n",
    "    # Enriquece os chunks de texto DEPOIS de finalizar metadata\n",
    "    if unique_chunks and isinstance(unique_chunks[0], dict) and \"error\" not in unique_chunks[0]:\n",
    "         try:\n",
    "             final_chunks = enrich_with_metadata(unique_chunks, metadata)\n",
    "         except Exception as enrich_error:\n",
    "              print(f\"Alerta: Erro ao enriquecer chunks de texto: {enrich_error}\")\n",
    "              final_chunks = [{\"error\": \"Falha ao enriquecer chunks de texto\", \"details\": str(enrich_error)}]\n",
    "    elif not unique_chunks:\n",
    "         final_chunks = []\n",
    "    else: # Caso unique_chunks já contenha um erro\n",
    "         final_chunks = unique_chunks \n",
    "\n",
    "\n",
    "    # --- MONTAGEM DO JSONL FINAL ---\n",
    "    documento_final = {\n",
    "        \"metadata\": metadata,\n",
    "        \"page_specific_data\": final_page_specific_data,\n",
    "        \"content_chunks\": final_chunks\n",
    "    }\n",
    "\n",
    "    # --- SAÍDA ---\n",
    "    output_path_jsonl = os.path.join(output_dir, f\"{file_name}.jsonl\")\n",
    "    try:\n",
    "        print(f\"---> PREPARANDO PARA SALVAR {file_name} (Tipo: {pdf_type}):\")\n",
    "        print(f\"     Metadata keys: {list(documento_final.get('metadata', {}).keys())}\")\n",
    "        print(f\"     page_specific_data length: {len(documento_final.get('page_specific_data', []))}\")\n",
    "        print(f\"     content_chunks length: {len(documento_final.get('content_chunks', []))}\")\n",
    "\n",
    "        with open(output_path_jsonl, 'w', encoding='utf-8') as f:\n",
    "            f.write(json.dumps(documento_final, ensure_ascii=False, indent=2)) # Salva formatado\n",
    "        print(f\"---> Arquivo JSONL salvo com sucesso (formatado com indentação): {output_path_jsonl}\")\n",
    "    except Exception as e:\n",
    "        print(f\"\\nERRO ao salvar o arquivo de saída '{output_path_jsonl}'. Detalhes: {e}\")\n",
    "\n",
    "# --- Fim do loop principal ---\n",
    "\n",
    "print(\"\\n----------------------------------------------------\")\n",
    "print(\"Processamento de todos os arquivos concluído!\")\n",
    "print(f\"Resultados salvos em: '{output_dir}'\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
